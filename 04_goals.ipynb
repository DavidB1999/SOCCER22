{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d23b7a3b",
   "metadata": {},
   "source": [
    "# 80 / 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91299fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as p\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# load the above and below 80th percentile split data\n",
    "with open('Pickles/80_20_split.pkl', 'rb') as file:\n",
    "    split80_20 = p.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eeed4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "split80_20.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2685dff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load the goal information\n",
    "goals = pd.read_excel('Tore.xlsx')\n",
    "goals.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7ed1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring_ha = which team scored\n",
    "\n",
    "goals['time_string'] = goals['Timestamp_point'].apply(str)\n",
    "goals.loc[goals['Scoring Team'] == goals['Home'], 'Scoring_ha'] = -3\n",
    "goals.loc[goals['Scoring Team'] == goals['Away'], 'Scoring_ha'] = -1\n",
    "goals.to_excel(\"goal_table.xlsx\")\n",
    "goals\n",
    "\n",
    "#df1.to_excel(\"output.xlsx\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f910610",
   "metadata": {},
   "source": [
    "## Mean success score for goals and no goals\n",
    "### We take all 45 goals as goals for 30m intervals and only the 39 within the box for 16m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497c27a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "import pandas as pd\n",
    "import pickle as p\n",
    "\n",
    "# load complete data\n",
    "with open('Pickles/AllDatapoints.pkl', 'rb') as file:\n",
    "    full_dict = p.load(file)\n",
    "    \n",
    "full_dict.keys()\n",
    "datasets = list(full_dict.keys())                            # datasets\n",
    "goal_dict = {}\n",
    "df_SS_goal = pd.DataFrame(index=['Goal', 'No_goal'])\n",
    "\n",
    "for dataset in datasets:\n",
    "    goal_dict[\"_\".join([dataset, 'GOAL'])] = []                  # for every previous dataset in my dictionary i now want two: \n",
    "    goal_dict[\"_\".join([dataset, 'NOgoal'])] = [] \n",
    "    for frame in range(len(full_dict[dataset])):                # for every frame\n",
    "        if dataset.startswith('30'):                                                # for the 30m intervals we don't need to check whether the goal was scored from within the penalty area\n",
    "            if float(full_dict[dataset][frame][6]) in goals.Timestamp_point.values:        # to identify goals the time has to match\n",
    "                index_list = np.where(goals[\"Timestamp_point\"].values == float(full_dict[dataset][frame][6]))      # all those which match by the time stamp\n",
    "                for i in index_list:\n",
    "                    if goals.Gamecode[i[0]] in full_dict[dataset][frame][2] and goals.Scoring_ha[i[0]] == full_dict[dataset][frame][1]:       # have to also match by the match id and the team (scoring team)\n",
    "                        goal_dict[\"_\".join([dataset, 'GOAL'])].append(full_dict[dataset][frame][0])         # if yes we append to the goal data set\n",
    "                        # print(split80_20[dataset][frame])\n",
    "                        # print(dataset)\n",
    "                    else:\n",
    "                        goal_dict[\"_\".join([dataset, 'NOgoal'])].append(full_dict[dataset][frame][0])      # else we append to the no goal data set\n",
    "            else: \n",
    "                goal_dict[\"_\".join([dataset, 'NOgoal'])].append(full_dict[dataset][frame][0])              # else we append to the no goal data set\n",
    "        elif dataset.startswith('16'):                                               # for 16m intervals we have to check if scored from within the penalty are\n",
    "            if float(full_dict[dataset][frame][6]) in goals.Timestamp_point[goals.David==1].values:        # to identify goals the time has to match (==1 --> from within the box)\n",
    "                index_list = np.where(goals[\"Timestamp_point\"].values == float(full_dict[dataset][frame][6]))      # all those which match by the time stamp\n",
    "                for i in index_list:\n",
    "                    if goals.Gamecode[i[0]] in full_dict[dataset][frame][2] and goals.Scoring_ha[i[0]] == full_dict[dataset][frame][1]:       # have to also match by the match id and the team (scoring team)\n",
    "                        goal_dict[\"_\".join([dataset, 'GOAL'])].append(full_dict[dataset][frame][0])         # if yes we append to the goal data set\n",
    "                        # print(split80_20[dataset][frame])\n",
    "                        # print(dataset)\n",
    "                    else:\n",
    "                        goal_dict[\"_\".join([dataset, 'NOgoal'])].append(full_dict[dataset][frame][0])      # else we append to the no goal data set\n",
    "            else: \n",
    "                goal_dict[\"_\".join([dataset, 'NOgoal'])].append(full_dict[dataset][frame][0])              # else we append to the no goal data set\n",
    "\n",
    "    print(dataset)\n",
    "    print(mean(goal_dict[\"_\".join([dataset, 'GOAL'])]))\n",
    "    print(mean(goal_dict[\"_\".join([dataset, 'NOgoal'])]))\n",
    "    df_SS_goal[dataset] = [mean(goal_dict[\"_\".join([dataset, 'GOAL'])]), mean(goal_dict[\"_\".join([dataset, 'NOgoal'])])]\n",
    "    df_SS_goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166f672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SS_goal.to_csv('Results/Success_Score_goal.csv')\n",
    "df_SS_goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3325afbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SS_perc = pd.DataFrame(index=['Goal', 'No_goal'])\n",
    "for dataset in datasets:\n",
    "    perc_mean = []\n",
    "    \n",
    "    for v in range(0,2):\n",
    "        perc = []\n",
    "        i = 0\n",
    "        for line in full_dict[dataset]:\n",
    "            i += 1\n",
    "            if line[0] > (df_SS_goal[dataset][v]-0.1) and line[0] < (df_SS_goal[dataset][v]+0.1):\n",
    "                print(dataset)\n",
    "                print(line)\n",
    "                print(len(full_dict[dataset]))\n",
    "                print(i/len(full_dict[dataset]))\n",
    "                print(i)\n",
    "                print('END OF THIS BLOCK')\n",
    "                print('')\n",
    "                perc.append(i/len(full_dict[dataset]))\n",
    "        perc_mean.append(mean(perc))\n",
    "    df_SS_perc[dataset] = perc_mean\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87ce571",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SS_perc.to_csv('Results/Success_Score_percentile.csv')\n",
    "df_SS_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95af17e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need a column to which subset each frame /SS belongs\n",
    "# so that i can then do what MS did and split goals scored on a frequent SS in different groups\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "with open('Pickles/AllDatapoints.pkl', 'rb') as file:            # import the dictionary where data is not split by group or 80 percentile\n",
    "    full_dict = p.load(file)\n",
    "\n",
    "with open('Pickles/index_limits_80.pkl', 'rb') as file:                 # import the boarders /limits (index) for over under 80\n",
    "    limits_80 = p.load(file)\n",
    "\n",
    "with open('Pickles/index_limits_8.pkl', 'rb') as file:                 # import the boarders /limits (index) for the 8 groups\n",
    "    limits_8 = p.load(file)\n",
    "    \n",
    "success_scores = {}                                      # create a dictionary to sort the success scores\n",
    "\n",
    "datasets = list(full_dict.keys())                    # for all datasets by area & interval (not split)  \n",
    "for dataset in datasets:\n",
    "    success_scores[dataset] = []                                   # create a success score vector \n",
    "    for frame in range(len(full_dict[dataset])):               # for every frame (by number, not the frame itself)     \n",
    "        success_scores[dataset].append(full_dict[dataset][frame][0])     # append the success score to the vector\n",
    "     \n",
    "    \n",
    "# by index determine wether a success score falls in under or over the 80th percentile\n",
    "\n",
    "for dataset in datasets:\n",
    "    success_scores[\"_\".join([dataset, 'ou'])] = np.repeat(\"NaN\", limits_80[dataset][1])\n",
    "    success_scores[\"_\".join([dataset, 'ou'])][:limits_80[dataset][0]] = 'U'    # up to the limit it is under = U\n",
    "    success_scores[\"_\".join([dataset, 'ou'])][limits_80[dataset][0]:] = 'O'    # Upwards of the limit it is over = O\n",
    "\n",
    "\n",
    "## by index determine the group a success score falls into (1-8)\n",
    "\n",
    "for dataset in datasets:\n",
    "    success_scores[\"_\".join([dataset, \"8\"])] = np.repeat(\"NaN\", limits_80[dataset][1])     # first create a vector with NaN (full length)\n",
    "    for i in range(len(limits_8[dataset])):                                               # fill it based on the read limits (s.above)\n",
    "        if i == 0:\n",
    "            success_scores[\"_\".join([dataset, \"8\"])][0:int(limits_8[dataset][i])] = i + 1    # between 0 and boarder of group 1\n",
    "        else:\n",
    "            success_scores[\"_\".join([dataset, \"8\"])][int(limits_8[dataset][i-1]):int(limits_8[dataset][i])] = i + 1  # between boarder of group -1 and group\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0360a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_scores.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8837fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_scores['16m_0010s_ou']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbd7e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_scores['16m_0010s_8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d91aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted (ascending)\n",
    "success_scores['16m_0010s']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a63702",
   "metadata": {},
   "source": [
    "# Goals over and under 80th percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29882cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_count = {}                                 # new dictionary to count the goals\n",
    "datasets = list(full_dict.keys())           # datasets in this case not split based on percentiles (area & interval)\n",
    "\n",
    "with open('Pickles/Boarder_values_10.pkl', 'rb') as file:             # boarder values for the 80th percentile (the 8th element in each array)\n",
    "    percentiles_10 = p.load(file)\n",
    "    \n",
    "\n",
    "for dataset in datasets:\n",
    "    goal_count[\"_\".join([dataset, 'over80'])] = 0                    # for every dataset i count goals over\n",
    "    goal_count[\"_\".join([dataset, 'under80'])] = 0                   # and under --> 2 lists in dictionary\n",
    "    for frame in range(len(full_dict[dataset])):                   # every frame is checked\n",
    "        if dataset.startswith('30'):\n",
    "            if float(full_dict[dataset][frame][6]) in goals.Timestamp_point.values:        #wether the time fits one of the goals\n",
    "                index_list = np.where(goals[\"Timestamp_point\"].values == float(full_dict[dataset][frame][6]))    # if yes we get the index of the goal\n",
    "                for i in index_list:\n",
    "                    #print(goal_count[\"_\".join([dataset, 'under80'])])\n",
    "                    #print(goal_count[\"_\".join([dataset, 'over80'])])\n",
    "                    if goals.Gamecode[i[0]] in full_dict[dataset][frame][2] and goals.Scoring_ha[i[0]] == full_dict[dataset][frame][1]:    # and check whether matchcode and scoring team fit as well --> confirm it is a goal frame\n",
    "                        if full_dict[dataset][frame][0] < percentiles_10[dataset][8]:                                               # if under the boarder value\n",
    "                            goal_count[\"_\".join([dataset, 'under80'])] = float(goal_count[\"_\".join([dataset, 'under80'])]) + 1 # we add + 1 to under count\n",
    "                        elif full_dict[dataset][frame][0] > percentiles_10[dataset][8]:                                             # if over the boarder value\n",
    "                            goal_count[\"_\".join([dataset, 'over80'])] = float(goal_count[\"_\".join([dataset, 'over80'])]) + 1   # we add +1  to over count\n",
    "                        elif full_dict[dataset][frame][0] == percentiles_10[dataset][8]:                                            # if excatly the boarder value\n",
    "                            tot_frequency = len(np.where(np.array(success_scores[dataset]) == full_dict[dataset][frame][0])[0])         # freqeuncy of frames with that success score\n",
    "                            over_frequency = len(np.where((np.array(success_scores[dataset]) == full_dict[dataset][frame][0]) & (np.array(success_scores[\"_\".join([dataset, \"ou\"])]) == 'O'))[0])         # freqeuncy of frames with that success score in over\n",
    "                            under_frequency = len(np.where((np.array(success_scores[dataset]) == full_dict[dataset][frame][0]) & (np.array(success_scores[\"_\".join([dataset, \"ou\"])]) == 'U'))[0])        # freqeuncy of frames with that success score in under\n",
    "                            o_goals = over_frequency / tot_frequency                                                              # goal share for over \n",
    "                            u_goals = under_frequency / tot_frequency                                                             # goal share for under\n",
    "                            goal_count[\"_\".join([dataset, 'under80'])] = float(goal_count[\"_\".join([dataset, 'under80'])]) + u_goals    # add the share\n",
    "                            #print(goal_count[\"_\".join([dataset, 'under80'])])\n",
    "                            goal_count[\"_\".join([dataset, 'over80'])] = float(goal_count[\"_\".join([dataset, 'over80'])]) + o_goals      # add the share\n",
    "        elif dataset.startswith('16'):\n",
    "            if float(full_dict[dataset][frame][6]) in goals.Timestamp_point[goals.David == 1].values:        #whether the time fits one of the goals (only the 39 in the box ==1)\n",
    "                index_list = np.where(goals[\"Timestamp_point\"].values == float(full_dict[dataset][frame][6]))    # if yes we get the index of the goal\n",
    "                for i in index_list:\n",
    "                    #print(goal_count[\"_\".join([dataset, 'under80'])])\n",
    "                    #print(goal_count[\"_\".join([dataset, 'over80'])])\n",
    "                    if goals.Gamecode[i[0]] in full_dict[dataset][frame][2] and goals.Scoring_ha[i[0]] == full_dict[dataset][frame][1]:    # and check whether matchcode and scoring team fit as well --> confirm it is a goal frame\n",
    "                        if full_dict[dataset][frame][0] < percentiles_10[dataset][8]:                                               # if under the boarder value\n",
    "                            goal_count[\"_\".join([dataset, 'under80'])] = float(goal_count[\"_\".join([dataset, 'under80'])]) + 1 # we add + 1 to under count\n",
    "                        elif full_dict[dataset][frame][0] > percentiles_10[dataset][8]:                                             # if over the boarder value\n",
    "                            goal_count[\"_\".join([dataset, 'over80'])] = float(goal_count[\"_\".join([dataset, 'over80'])]) + 1   # we add +1  to over count\n",
    "                        elif full_dict[dataset][frame][0] == percentiles_10[dataset][8]:                                            # if excatly the boarder value\n",
    "                            tot_frequency = len(np.where(np.array(success_scores[dataset]) == full_dict[dataset][frame][0])[0])         # freqeuncy of frames with that success score\n",
    "                            over_frequency = len(np.where((np.array(success_scores[dataset]) == full_dict[dataset][frame][0]) & (np.array(success_scores[\"_\".join([dataset, \"ou\"])]) == 'O'))[0])         # freqeuncy of frames with that success score in over\n",
    "                            under_frequency = len(np.where((np.array(success_scores[dataset]) == full_dict[dataset][frame][0]) & (np.array(success_scores[\"_\".join([dataset, \"ou\"])]) == 'U'))[0])        # freqeuncy of frames with that success score in under\n",
    "                            o_goals = over_frequency / tot_frequency                                                              # goal share for over \n",
    "                            u_goals = under_frequency / tot_frequency                                                             # goal share for under\n",
    "                            goal_count[\"_\".join([dataset, 'under80'])] = float(goal_count[\"_\".join([dataset, 'under80'])]) + u_goals    # add the share\n",
    "                            #print(goal_count[\"_\".join([dataset, 'under80'])])\n",
    "                            goal_count[\"_\".join([dataset, 'over80'])] = float(goal_count[\"_\".join([dataset, 'over80'])]) + o_goals      # add the share\n",
    "\n",
    "                        \n",
    "# create a dataframe storing the count of goals in a practical format and save to csv\n",
    "goal_count_80_df = pd.DataFrame() \n",
    "\n",
    "\n",
    "stri = ['over80', 'under80']\n",
    "strin = ['over', 'under']\n",
    "for dataset in datasets:\n",
    "    counts = []\n",
    "    for i in range(len(stri)):\n",
    "        counts.append(goal_count[\"_\".join([dataset, stri[i]])])\n",
    "        counts.append(len(split80_20[\"_\".join([dataset, strin[i]])]))        # add the length of the data for chi square\n",
    "    counts.append(len(success_scores[dataset]))\n",
    "    goal_count_80_df[dataset] = counts\n",
    "    \n",
    "\n",
    "\n",
    "goal_count_80_df.to_csv('Results/goal_Count_80.csv')\n",
    "goal_count_80_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efd42e6",
   "metadata": {},
   "source": [
    "# Goals in 8 groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8da6bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the 8 groups split data\n",
    "with open('Pickles/8_split.pkl', 'rb') as file:\n",
    "    split_8 = p.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc3d036",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_8.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25bbbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "limits_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331dae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_count_8 = {}\n",
    "datasets = list(full_dict.keys())\n",
    "\n",
    "\n",
    "with open('Pickles/Boarder_values_8.pkl', 'rb') as file:\n",
    "    boarder_values_8 = p.load(file)\n",
    "    \n",
    "groups =list(range(1,9))\n",
    "\n",
    "tot_frequency_dict = {}\n",
    "group_frequencies_dict = {}\n",
    "goal_share = {}\n",
    "\n",
    "for dataset in datasets:                                   # for each combination of area & interval\n",
    "    for group in groups:                                   # for each group (by percentile) --> 1 to 8\n",
    "        goal_count_8[\"_\".join([dataset, str(group)])] = 0  # empty list in the dictionary for every group for every dataset\n",
    "        tot_frequency_dict[\"_\".join([dataset, str(group)])] = []         # dictionary for the total frequency of that success score\n",
    "        group_frequencies_dict[\"_\".join([dataset, str(group)])] = []     # dictionary for the freqeuncy of that success score in that group\n",
    "        goal_share[\"_\".join([dataset, str(group)])] = []                 # dictionary for the share of the goals scored at that success score for each group\n",
    "    for frame in range(len(full_dict[dataset])):       # for every frame (loop by number; not the frame itself)\n",
    "        if (float(full_dict[dataset][frame][6]) in goals.Timestamp_point.values and dataset.startswith('30')) or (float(full_dict[dataset][frame][6]) in goals.Timestamp_point[goals.David==1].values):             # is the time in the frame in the list of goals (-->potential goal frame)\n",
    "            # either a 30m datapoint with the right time stamp or a 16m datapoint with the right time stamp matching only the 39\n",
    "            index_list = np.where(goals[\"Timestamp_point\"].values == float(full_dict[dataset][frame][6]))      # create a list the index of the goal in the goal data frame\n",
    "            for i in index_list:                                                                # for every goal with that time\n",
    "                if goals.Gamecode[i[0]] in full_dict[dataset][frame][2] and goals.Scoring_ha[i[0]] == full_dict[dataset][frame][1]:  # is the gamecode identical with the frame and the scoring team the correct one\n",
    "                    for group in groups:                   # for each group (by percentile) --> 1 to 8\n",
    "                        if group == 1:                                                   # if group is = 1 group-1 is not usable for lower limit so we here have to use 0 - group1 \n",
    "                            if full_dict[dataset][frame][0] < boarder_values_8[dataset][group-1]:                   # is the success score smaller than the limit of group1\n",
    "                                goal_count_8[\"_\".join([dataset, str(group)])] = goal_count_8[\"_\".join([dataset, str(group)])] + 1    # if yes it counts as a full goal to goals in group \n",
    "                            elif full_dict[dataset][frame][0] == boarder_values_8[dataset][group-1]:   # if not we check if it is exactly the boarder value; if yes we create a share based on the frequency of that success score in each group\n",
    "                                tot_frequency_dict[\"_\".join([dataset, str(group)])] = len(np.where(np.array(success_scores[dataset]) == full_dict[dataset][frame][0])[0])   # the tot_frequency is frequency of that success score across all groups\n",
    "                                #print(tot_frequency_dict[\"_\".join([dataset, str(group)])])\n",
    "                                group_frequencies_dict[\"_\".join([dataset, str(group)])] = len(np.where((np.array(success_scores[dataset]) == full_dict[dataset][frame][0]) & (np.array(success_scores[\"_\".join([dataset, \"8\"])]) == str(group)))[0])  # the freqeuncy of that success score in this group\n",
    "                                #print(group_frequencies_dict[\"_\".join([dataset, str(group)])])\n",
    "                                goal_share[\"_\".join([dataset, str(group)])] = group_frequencies_dict[\"_\".join([dataset, str(group)])]/tot_frequency_dict[\"_\".join([dataset, str(group)])]   # goal share = frequency in that group / overall frequency (* goals with that success score)\n",
    "                                #print(goal_count_8[\"_\".join([dataset, str(group)])])\n",
    "                                goal_count_8[\"_\".join([dataset, str(group)])] = float(goal_count_8[\"_\".join([dataset, str(group)])]) + goal_share[\"_\".join([dataset, str(group)])]      # finally we add that share to that group\n",
    "                                #print(goal_share[\"_\".join([dataset, str(group)])])\n",
    "                                #print(goal_count_8[\"_\".join([dataset, str(group)])])\n",
    "                        else:\n",
    "                            if full_dict[dataset][frame][0] < boarder_values_8[dataset][group-1] and full_dict[dataset][frame][0] > boarder_values_8[dataset][group-2]: # if group is > 1 we can just check with limits (group -1 upper limit to group upper limit)\n",
    "                                goal_count_8[\"_\".join([dataset, str(group)])] = goal_count_8[\"_\".join([dataset, str(group)])] + 1                   # if yes it counts as a full goal to goals in group \n",
    "                            elif (full_dict[dataset][frame][0] == boarder_values_8[dataset][group-1]) | (full_dict[dataset][frame][0] == boarder_values_8[dataset][group-2]):                      # if not we check if it is exactly the boarder value; if yes we create a share based on the frequency of that success score in each group\n",
    "                                tot_frequency_dict[\"_\".join([dataset, str(group)])] = len(np.where(np.array(success_scores[dataset]) == full_dict[dataset][frame][0])[0])                           # the tot_frequency is frequency of that success score across all groups\n",
    "                                group_frequencies_dict[\"_\".join([dataset, str(group)])] = len(np.where((np.array(success_scores[dataset]) == full_dict[dataset][frame][0]) & (success_scores[\"_\".join([dataset, \"8\"])] == str(group)))[0]) # the tot_frequency is frequency of that success score across all groups\n",
    "                                goal_share[\"_\".join([dataset, str(group)])]= group_frequencies_dict[\"_\".join([dataset, str(group)])]/tot_frequency_dict[\"_\".join([dataset, str(group)])]           # goal share = frequency in that group / overall frequency (* goals with that success score)\n",
    "                                goal_count_8[\"_\".join([dataset, str(group)])] = float(goal_count_8[\"_\".join([dataset, str(group)])]) + goal_share[\"_\".join([dataset, str(group)])]                # finally we add that share to that group                                   \n",
    "                                #print(goal_count_8[\"_\".join([dataset, str(group)])])\n",
    "\n",
    "                                \n",
    "# create a dataframe storing the count of goals in a practical format and save to csv\n",
    "goal_count_8_df = pd.DataFrame() \n",
    "\n",
    "for dataset in datasets:\n",
    "    counts = []\n",
    "    for i in list(range(1,9)):\n",
    "        counts.append(goal_count_8[\"_\".join([dataset, str(i)])])\n",
    "    goal_count_8_df[dataset] = counts\n",
    "    \n",
    "\n",
    "\n",
    "goal_count_8_df.to_csv('Results/goal_Count_8.csv')\n",
    "goal_count_8_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d3d394",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "floodlight_env",
   "language": "python",
   "name": "floodlight_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

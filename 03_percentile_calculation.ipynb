{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e0cf1cb",
   "metadata": {},
   "source": [
    "# Percentile calculation\n",
    "We need:\n",
    "+ the sorted list of all frames \n",
    "+ the limits of percentile 80 and the 8 groups as index\n",
    "+ the boarder values of percentile 80 and the 8 groups\n",
    "\n",
    "Preferably this is done with no additional random sorting of frames with identical success scores, since that would be redundant considering that we later split goals with a certain success score to all groups that contain some frames with that success score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040c5681",
   "metadata": {},
   "source": [
    "# Over and under the 80th percentile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0d253b",
   "metadata": {},
   "source": [
    "# If we do not want to randomize (cause Martin Stoller did not and it is redundant if we calculate a goal share as described in MA Stoller):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97bb670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a dictionary with one array for each dataset (area & interval)\n",
    "# These arrays contain each frame (as a list) entailing:\n",
    "# Success score, home (-3) or away (-1), string filename, minute and second, the half and the time as a string in a practical format\n",
    "# The array for every dataset is sorted by Success score\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "        \n",
    "#1:\n",
    "game_data = (r'game_data')\n",
    "\n",
    "# import the datasets\n",
    "with open(\"Pickles/Datasets.pkl\", \"rb\") as l:\n",
    "    datasets = pickle.load(l)\n",
    "\n",
    "\n",
    "final_dict_nR ={}                                               # create an empty dictionary\n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    final_dict_nR[datasets[i]] = []                              # datasets as keys\n",
    "\n",
    "for dataset in datasets:\n",
    "    for dir in os.listdir(game_data):\n",
    "        loc = os.path.join(game_data, dir)\n",
    "        # os.chdir(loc)                                         # setting the working directory to data folder causes chaos with checkpoints\n",
    "        for file in os.listdir(loc):\n",
    "            if file.startswith('zero') & file.endswith(\"postprocessed.csv\"):\n",
    "                if dataset in file:                                # Check if its a file for the current dataset (area & interval length)\n",
    "                    with open(os.path.join(loc, file), 'r') as f:\n",
    "                        reader = csv.reader(f, delimiter =',')\n",
    "                        for row in reader:\n",
    "                            if row != []:\n",
    "                                final_dict_nR[dataset].append([float(row[-1]), -1, file, row[0], row[1]])     # index of filename needs to be adapted later\n",
    "                                final_dict_nR[dataset].append([float(row[-3]), -3, file, row[0], row[1]])     # content of dictionary see above (1.)\n",
    "                                                                                                        # (-3 = home, -1 = away),\n",
    "                                \n",
    "for dataset in datasets:\n",
    "    for i in range(len(final_dict_nR[dataset])):\n",
    "        if 'HZ1' in final_dict_nR[dataset][i][2]:                    # appending a half time indicator \n",
    "            final_dict_nR[dataset][i].append(float(1))               # if HZ1 is in file name we append a one\n",
    "        elif 'HZ2' in final_dict_nR[dataset][i][2]:                  # if HZ2 is in file name \n",
    "            final_dict_nR[dataset][i].append(float(2))               # we append a 2\n",
    "        else:\n",
    "            final_dict_nR[dataset][i].append('Error')                # if none we append Error (which we use to check later)\n",
    "        \n",
    "        if len(str(final_dict_nR[dataset][i][4])) == 1:              # if the seconds is just one digit (less than 10s) we need\n",
    "            proxy_s = str(0) + str(final_dict_nR[dataset][i][4])     # to add a 0 to get a format that allows us to compare with SOCCER output\n",
    "        else:\n",
    "            proxy_s = final_dict_nR[dataset][i][4]\n",
    "            \n",
    "        if final_dict_nR[dataset][i][5] == 1:                        # if it is the first half we just combine minutes and seconds\n",
    "            final_dict_nR[dataset][i].append(\".\".join([final_dict_nR[dataset][i][3], proxy_s]))   # into one variable\n",
    "        elif final_dict_nR[dataset][i][5] == 2:                      # if it is the second half we add 45 to the minutes\n",
    "            proxy_m = str(int(final_dict_nR[dataset][i][3]) + 45)      # and then combine them the same way\n",
    "            final_dict_nR[dataset][i].append(\".\".join([proxy_m, proxy_s]))\n",
    "             \n",
    "    final_dict_nR[dataset].sort()               # sort by success score primarily and the random order secondly \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11931aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    print('Error' in final_dict_nR[dataset])                         # checking if halftime indicator worked without Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c84a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.2 get average success score\n",
    "for dataset in datasets:\n",
    "    success_scores = []   \n",
    "    for datapoint in final_dict_nR[dataset]:\n",
    "        success_scores.append(datapoint[0])\n",
    "    print('Average for ' + dataset + ': ' + str(sum(success_scores)/len(success_scores)))\n",
    "    #print(len(success_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546cc54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy alternative to get percentile border values\n",
    "# Saving the whole bunch to use the 80th percentile values later\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "percentiles_10 = {}\n",
    "for dataset in datasets:\n",
    "    percentiles_10[dataset] = []\n",
    "    values = []\n",
    "    for line in final_dict_nR[dataset]:\n",
    "        values.append(line[0])\n",
    "    percentiles_10[dataset] = np.percentile(values, [0,10,20,30,40,50,60,70,80,90,100])          # saving the boarder values in a dicitonary\n",
    "\n",
    "\n",
    "with open(\"Pickles/Boarder_values_10.pkl\", \"wb\") as dict:                # saving the boarder values in a dicitonary\n",
    "    pickle.dump(percentiles_10, dict)\n",
    "        \n",
    "percentiles_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e1c720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dictionary into over and under 80 resulting in twice as many lists as datasets (dataset_over, dataset_under)\n",
    "# simultaneously storing limit indexes \n",
    "\n",
    "import math\n",
    "split_dict_nR = {}                           # dicitonary where we split the datasets in over and under the 80 --> 2 arrays (over and under) per dataset (area & interval)\n",
    "limits_80 = {}                               # simultaneously storing the limits (index) as well as the total length for use later\n",
    "for dataset in datasets:\n",
    "    limits_80[dataset] = []\n",
    "    length = (len(final_dict_nR[dataset]))           # the 80th percentile (when sorted) is just the borderline of 80% of the values\n",
    "    print(length)\n",
    "    percentile = length * 0.8                  # get 80% length of the values\n",
    "    #print(percentile)\n",
    "    upper =round(percentile)          # substract 1 since index counting starts with zero and add one because the last one is not included :5 --> 1,2,3,4\n",
    "    print(upper)\n",
    "    lower = round(percentile)          # upper and lower are the same index as the last index is not included when selecting :5 --> 1,2,3,4\n",
    "    print(lower)\n",
    "    limits_80[dataset].append(upper)       # append the limit index\n",
    "    limits_80[dataset].append(length)      # and the total length\n",
    "    split_dict_nR[\"_\".join([dataset, 'under'])] = final_dict_nR[dataset][:upper]\n",
    "    split_dict_nR[\"_\".join([dataset, 'over'])] = final_dict_nR[dataset][lower:]\n",
    "    \n",
    "with open(\"Pickles/index_limits_80.pkl\", \"wb\") as dict:\n",
    "    pickle.dump(limits_80, dict)\n",
    "\n",
    "limits_80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ffccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dict_nR.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c1b3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(split_dict_nR['30m_0010s_over'])\n",
    "\n",
    "print(split_dict_nR['30m_0010s_over'][0])\n",
    "print(split_dict_nR['30m_0010s_over'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a97715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(split_dict_nR['30m_0010s_under']))\n",
    "\n",
    "print(split_dict_nR['30m_0010s_under'][0])\n",
    "print(split_dict_nR['30m_0010s_under'][50000])\n",
    "print(split_dict_nR['30m_0010s_under'][75000])\n",
    "print(split_dict_nR['30m_0010s_under'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff98cab2",
   "metadata": {},
   "source": [
    "# 8 groups &rarr; percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f73097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb5b3dc",
   "metadata": {},
   "source": [
    "**length** return the **number of cases** which is **one more** than the last index since 0 is the index of the first case. But since index :x return everything up to x but **excluding x** the length should be the upper limit of the percentile 87.5 - 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9651632c",
   "metadata": {},
   "source": [
    "# if we do not want to randomize we chose this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18162892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into the 8 groups --> 8 lists (by percentiles) per dataset (area & interval)\n",
    "# saving the index limits and the success score boarder values\n",
    "\n",
    "import math\n",
    "\n",
    "percentiles = [0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 1]\n",
    "split8_dict_nR = {}\n",
    "limits_8 = {}\n",
    "boarder_values_8 = {}\n",
    "lower_lim = 0\n",
    "for dataset in datasets:\n",
    "    limits_8[dataset] = []\n",
    "    boarder_values_8[dataset] = []\n",
    "    lower_lim = 0\n",
    "    length = (len(final_dict_nR[dataset]))           # the 80th percentile (when sorted) is just the borderline of 80% of the values\n",
    "    for p in range(len(percentiles)):\n",
    "        percentile = round(length * percentiles[p])            # get percentile, round uses symetrical rounding (.5 --> up or down to go to even number (3.5 --> 4, 2.5 --> 2))\n",
    "        upper_lim = percentile                                 # this becomes the upper limit\n",
    "        limits_8[dataset].append(upper_lim)\n",
    "        boarder_values_8[dataset].append(final_dict_nR[dataset][upper_lim-1][0])\n",
    "        print(str(lower_lim) +' - ' +  str(upper_lim))         # just to check we print our intervals\n",
    "        \n",
    "        split8_dict_nR[\"_\".join([dataset, str(percentiles[p])])] = final_dict_nR[dataset][lower_lim:upper_lim]     # and then split by these intervals\n",
    "#         if p == 7:                         \n",
    "#             lower_lim = 0                                      # see below except if its the last round in this loop then we need to start with 0 next time\n",
    "#         else:\n",
    "#             lower_lim = upper_lim                              # for the next round in this loop the upper limit of this round become the lower limit for the next\n",
    "        lower_lim = upper_lim                                    # solved it more elegantly with settting lower_lim in the loop on layer above\n",
    "limits_8 \n",
    "\n",
    "with open(\"Pickles/index_limits_8.pkl\", \"wb\") as dict:\n",
    "    pickle.dump(limits_8, dict)\n",
    "\n",
    "with open(\"Pickles/Boarder_values_8.pkl\", \"wb\") as dict:\n",
    "    pickle.dump(boarder_values_8, dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86337c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "boarder_values_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9570adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "split8_dict_nR.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e4a8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in split8_dict_nR.keys():\n",
    "    print(key)\n",
    "    print(len(split8_dict_nR[key]))     # the subsets of each dataset should have the same length which is rouned to 1/8th\n",
    "# see next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b883897",
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in final_dict_nR:\n",
    "    print(len(final_dict_nR[elem]))\n",
    "    print(len(final_dict_nR[elem]) / 8)\n",
    "    \n",
    "# this is how long the subsets should be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea2113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum, maximum and most importantly mean success score for every percentile group of every dataset\n",
    "# means stored as csv\n",
    "\n",
    "import pandas as pd\n",
    "percentiles = [0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 1]\n",
    "dfs_nR = {}\n",
    "means_dict = {}\n",
    "    \n",
    "for data in split8_dict_nR:                          # for every subset (8 x #datasets)\n",
    "    dfs_nR[data] = pd.DataFrame(split8_dict_nR[data], columns=['Success Score', 'Home_away', 'csv_file', 'min', 'sec', 'half', 'time_weird_format'])  # creating a dictionary of dataframes\n",
    "    minimum = dfs_nR[data]['Success Score'].min()\n",
    "    maximum = dfs_nR[data]['Success Score'].max()\n",
    "    mean = dfs_nR[data]['Success Score'].mean()\n",
    "    means_dict[data] = mean\n",
    "    print(data + ';' + str(minimum) + '-' +str(maximum) + ';' +str(mean))\n",
    "\n",
    "means_df = pd.DataFrame()\n",
    "   \n",
    "for dataset in datasets:\n",
    "    mean_v = []\n",
    "    for i in percentiles:\n",
    "        mean_v.append(means_dict[\"_\".join([dataset, str(i)])])\n",
    "    means_df[dataset] = mean_v\n",
    "    \n",
    "\n",
    "means_df.to_csv('Results/Mean_SuccessScores_8.csv')    \n",
    "means_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6663cd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "    \n",
    "with open(\"Pickles/80_20_split.pkl\", \"wb\") as cdict:\n",
    "    pickle.dump(split_dict_nR, cdict)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dff8a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"Pickles/AllDatapoints.pkl\", \"wb\") as cdict:\n",
    "    pickle.dump(final_dict_nR, cdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea1cffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "\n",
    "with open(\"Pickles/8_split.pkl\", \"wb\") as cdict:\n",
    "    pickle.dump(split8_dict_nR, cdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3baee2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
